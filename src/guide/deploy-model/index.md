# 部署模型推理服务

这一部分的任务涉及将深度学习模型部署为推理服务。在这些任务中，你将部署时下流行的模型并使用到多种高级功能，具体包括：

1. 使用平台提供的 [MLService](../../api/service/mlservice.md) 和 [SimpleMLService](../../api/service/simplemlservice.md) API，在多种推理框架下进行部署
1. 通过安装 [Apps](../../app/index.md) 来快速搭建 LLM 推理服务
1. 通过可视化界面查看推理服务信息，实时监控计算资源使用和性能指标
1. 实施多版本发布和金丝雀发布策略，确保服务的平稳过渡和风险控制
1. 配置自动伸缩机制，动态调整服务容量以适应负载变化，优化计算资源使用
