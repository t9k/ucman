# 部署模型推理服务

这一部分的任务涉及将深度学习模型部署为推理服务。在这些任务中，你将部署时下流行的模型并使用到多种高级功能，具体包括：

1. 使用平台提供的 <a target="_blank" rel="noopener noreferrer" href="https://t9k.github.io/user-manuals/latest/modules/deployment/mlservice.html">MLService</a> 和 <a target="_blank" rel="noopener noreferrer" href="https://t9k.github.io/user-manuals/latest/modules/deployment/simplemlservice.html">SimpleMLService</a> API，在多种推理框架下进行部署
1. 通过部署[应用](../../app/index.md)来快速搭建 LLM 推理服务
1. 通过可视化界面查看推理服务信息，实时监控计算资源使用和性能指标
1. 实施多版本发布和金丝雀发布策略，确保服务的平稳过渡和风险控制
1. 配置自动伸缩机制，动态调整服务容量以适应负载变化，优化计算资源使用
